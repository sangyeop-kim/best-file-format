{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools as it\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_read_time(filename, start):\n",
    "    time_length = time() - start\n",
    "    print(filename, '%.4f' % time_length)\n",
    "    with open('record_read.txt', 'a') as f:\n",
    "        f.write('%s : %.4f sec' % (filename, time_length) + '\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(extension, hyperparameter = None) :\n",
    "    '''\n",
    "    pd.to_csv, compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'\n",
    "    pd.to_json, orient : {'split', 'records', 'index', 'columns', 'values', 'table'}, \n",
    "                        default 'columns' \n",
    "                compression{'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'\n",
    "    pd.to_excel, None \n",
    "    pd.to_hdf, complib : {'zlib', 'lzo', 'bzip2', 'blosc'}, default 'zlib', \n",
    "                    format : {'fixed', 'table', None}, default 'fixed',\n",
    "    pd.to_feather, None\n",
    "    pd.to_parquet, compression{'snappy', 'gzip', 'brotli', None}, default 'snappy'\n",
    "    pd.to_stata, None\n",
    "    pd.to_pickle, compression{'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'\n",
    "    '''\n",
    "    \n",
    "    if hyperparameter is not None:\n",
    "        if type(hyperparameter) == str:\n",
    "            filename = hyperparameter\n",
    "        else:\n",
    "            not_none_hyperparameter = [i for i in hyperparameter if i is not None]\n",
    "            filename = '_'.join(not_none_hyperparameter)\n",
    "        \n",
    "        filename = 'test_%s.%s' % (filename,extension)\n",
    "    else:\n",
    "        filename = 'test.%s' % (extension)\n",
    "\n",
    "        \n",
    "    if extension == 'h5':\n",
    "        start = time()\n",
    "        df = pd.read_hdf(filename,'key', complib = hyperparameter[1])\n",
    "        measure_read_time(filename, start)\n",
    "        \n",
    "    elif extension == 'xlsx':\n",
    "        start = time()\n",
    "        df = pd.read_excel(filename)\n",
    "        measure_read_time(filename, start)\n",
    "        \n",
    "    elif extension == 'csv':\n",
    "        start = time()\n",
    "        df = pd.read_csv(filename, compression = hyperparameter)\n",
    "        measure_read_time(filename, start)\n",
    "        \n",
    "    elif extension == 'json':\n",
    "        start = time()\n",
    "        df = pd.read_json(filename, orient=hyperparameter[0], compression=hyperparameter[1])\n",
    "        measure_read_time(filename, start)\n",
    "        \n",
    "    elif extension == 'ftr':\n",
    "        start = time()\n",
    "        df = pd.read_feather(filename)\n",
    "        measure_read_time(filename, start)\n",
    "        \n",
    "    elif extension == 'parquet':\n",
    "        start = time()\n",
    "        df = pd.read_parquet(filename)\n",
    "        measure_read_time(filename, start)\n",
    "        \n",
    "    elif extension == 'dta':\n",
    "        start = time()\n",
    "        df = pd.read_stata(filename)\n",
    "        measure_read_time(filename, start)\n",
    "        \n",
    "    elif extension == 'pkl':\n",
    "        start = time()\n",
    "        df = pd.read_pickle(filename, compression = hyperparameter)\n",
    "        measure_read_time(filename, start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "json={'orient' : ['split', 'records', 'index', 'columns', 'values', 'table'], \n",
    "      'compression' : ['infer', 'gzip', 'bz2', 'zip', 'xz', None]}\n",
    "h5 = {'format' : ['fixed', 'table', None], 'complib' : ['zlib', 'lzo', 'bzip2', 'blosc']}\n",
    "\n",
    "json = list(it.product(*(json[Name] for Name in json.keys())))\n",
    "csv = ['infer', 'gzip', 'bz2', 'zip', 'xz', None]\n",
    "h5 = list(it.product(*(h5[Name] for Name in h5.keys())))\n",
    "parquet = ['snappy', 'gzip', 'brotli', None]\n",
    "pkl = ['infer', 'gzip', 'bz2', 'zip', 'xz', None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.ftr 0.1319\n"
     ]
    }
   ],
   "source": [
    "read_file('ftr', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_infer.pkl 0.9683\n",
      "test_gzip.pkl 2.3499\n",
      "test_bz2.pkl 14.9036\n",
      "test_zip.pkl 2.8101\n",
      "test_xz.pkl 4.9856\n",
      "test.pkl 0.9685\n"
     ]
    }
   ],
   "source": [
    "for i in pkl:\n",
    "    read_file('pkl', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.dta 6.0826\n"
     ]
    }
   ],
   "source": [
    "read_file('dta', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_snappy.parquet 0.1641\n",
      "test_gzip.parquet 0.3675\n",
      "test_brotli.parquet 0.1962\n",
      "test.parquet 0.1309\n"
     ]
    }
   ],
   "source": [
    "for i in parquet:\n",
    "    read_file('parquet', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_split_infer.json 30.6063\n",
      "test_split_gzip.json 34.3981\n",
      "test_split_bz2.json 49.7849\n",
      "test_split_zip.json 32.4178\n",
      "test_split_xz.json 37.6870\n",
      "test_split.json 30.6260\n",
      "test_records_infer.json 52.7509\n",
      "test_records_gzip.json 58.0450\n",
      "test_records_bz2.json 86.6441\n",
      "test_records_zip.json 58.0955\n",
      "test_records_xz.json 58.2658\n",
      "test_records.json 50.8276\n",
      "test_index_infer.json 146.1879\n",
      "test_index_gzip.json 153.0526\n",
      "test_index_bz2.json 186.2147\n",
      "test_index_zip.json 152.6008\n",
      "test_index_xz.json 156.6578\n",
      "test_index.json 145.6078\n",
      "test_columns_infer.json 87.5633\n",
      "test_columns_gzip.json 89.5058\n",
      "test_columns_bz2.json 118.7542\n",
      "test_columns_zip.json 88.7969\n",
      "test_columns_xz.json 99.0491\n",
      "test_columns.json 79.6321\n",
      "test_values_infer.json 26.3524\n",
      "test_values_gzip.json 29.8537\n",
      "test_values_bz2.json 45.2295\n",
      "test_values_zip.json 28.2081\n",
      "test_values_xz.json 33.9267\n",
      "test_values.json 27.5413\n",
      "test_table_infer.json 52.3023\n",
      "test_table_gzip.json 58.3693\n",
      "test_table_bz2.json 88.4866\n",
      "test_table_zip.json 56.9743\n",
      "test_table_xz.json 62.3319\n",
      "test_table.json 51.6060\n"
     ]
    }
   ],
   "source": [
    "for i in json:\n",
    "    read_file('json', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_infer.csv 10.1735\n",
      "test_gzip.csv 13.9003\n",
      "test_bz2.csv 36.7044\n",
      "test_zip.csv 13.6252\n",
      "test_xz.csv 20.0898\n",
      "test.csv 9.8901\n"
     ]
    }
   ],
   "source": [
    "for i in csv:\n",
    "    read_file('csv', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_fixed_zlib.h5 1.1123\n",
      "test_fixed_lzo.h5 1.0547\n",
      "test_fixed_bzip2.h5 1.0491\n",
      "test_fixed_blosc.h5 1.0509\n",
      "test_table_zlib.h5 4.3380\n",
      "test_table_lzo.h5 4.3472\n",
      "test_table_bzip2.h5 4.3019\n",
      "test_table_blosc.h5 4.3024\n",
      "test_zlib.h5 0.9725\n",
      "test_lzo.h5 0.9630\n",
      "test_bzip2.h5 1.0863\n",
      "test_blosc.h5 0.9703\n"
     ]
    }
   ],
   "source": [
    "for i in h5:\n",
    "    read_file('h5', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_sangyeop)",
   "language": "python",
   "name": "conda_sangyeop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
